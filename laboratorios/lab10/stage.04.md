# üîπ Fase 4 ‚Äî Modelos simples con Scikit-learn

### üéØ Objetivo

Tomar los datos procesados en la **Fase 3 (ETL con Pandas)**, aplicar un **preprocesado**, y construir **dos modelos simples**:

1. **Clasificaci√≥n supervisada**: predecir si una venta es ‚Äúalta‚Äù o ‚Äúbaja‚Äù seg√∫n el importe.
2. **Regresi√≥n supervisada**: estimar el precio de un producto en funci√≥n de la cantidad.

---

## üß± Estructura m√≠nima

```
lab10_db_ml/
‚îú‚îÄ data/export/
‚îÇ   ‚îú‚îÄ clientes.csv
‚îÇ   ‚îú‚îÄ productos.csv
‚îÇ   ‚îú‚îÄ ventas.csv
‚îÇ   ‚îî‚îÄ (reportes generados en fase 3)
‚îî‚îÄ app/ml_models.py
```

---

## üß≠ C√≥digo (app/ml\_models.py)

```python
# app/ml_models.py
from __future__ import annotations
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, LinearRegression

EXPORT_DIR = Path("data/export")

def load_dataset() -> pd.DataFrame:
    """Carga los CSV exportados en Fase 3 y prepara un DataFrame consolidado."""
    c = pd.read_csv(EXPORT_DIR / "clientes.csv").rename(columns={"id": "id_cliente"})
    p = pd.read_csv(EXPORT_DIR / "productos.csv").rename(columns={"id": "id_producto", "nombre": "producto"})
    v = pd.read_csv(EXPORT_DIR / "ventas.csv")

    df = (v.merge(c, on="id_cliente", how="left")
            .merge(p, on="id_producto", how="left"))
    df["importe"] = df["cantidad"] * df["precio"]
    return df

def clasificacion_importe(umbral: float = 200.0) -> float:
    """Clasificaci√≥n binaria: importe alto (>= umbral) vs bajo (< umbral)."""
    df = load_dataset()
    df["alto"] = (df["importe"] >= umbral).astype(int)

    X = df[["precio", "cantidad"]].values
    y = df["alto"].values

    scaler = StandardScaler()
    Xn = scaler.fit_transform(X)

    Xtr, Xte, ytr, yte = train_test_split(Xn, y, test_size=0.3, random_state=42)
    model = LogisticRegression()
    model.fit(Xtr, ytr)

    acc = model.score(Xte, yte)
    return acc

def regresion_precio() -> tuple[float, float, float, float]:
    """Regresi√≥n lineal: predecir precio a partir de la cantidad."""
    df = load_dataset()
    X = df[["cantidad"]].values
    y = df["precio"].values

    reg = LinearRegression()
    reg.fit(X, y)

    r2 = reg.score(X, y)
    coef = reg.coef_[0]
    inter = reg.intercept_
    pred_10 = reg.predict([[10]])[0]  # predicci√≥n para cantidad=10
    return r2, coef, inter, pred_10

def main():
    print("== Clasificaci√≥n: importe alto/bajo ==")
    acc = clasificacion_importe()
    print(f"Accuracy (importe >= 200): {acc:.2f}")

    print("\n== Regresi√≥n: precio en funci√≥n de cantidad ==")
    r2, coef, inter, pred = regresion_precio()
    print(f"R¬≤ = {r2:.2f}")
    print(f"Coef = {coef:.3f}, Intercepto = {inter:.3f}")
    print(f"Predicci√≥n para cantidad=10 ‚Üí {pred:.2f}")

    print("\n‚úî Fase 4 completada.")

if __name__ == "__main__":
    main()
```

---

## ‚ñ∂Ô∏è Ejecuci√≥n

```bash
python -m app.ml_models
```

**Salida esperada (aprox.):**

```
== Clasificaci√≥n: importe alto/bajo ==
Accuracy (importe >= 200): 1.00

== Regresi√≥n: precio en funci√≥n de cantidad ==
R¬≤ = 0.85
Coef = 15.000, Intercepto = 120.000
Predicci√≥n para cantidad=10 ‚Üí 270.00

‚úî Fase 4 completada.
```

*(Los valores exactos dependen de tus datos en `ventas`.)*

---

## ‚úÖ Criterios de aceptaci√≥n

* El script entrena un **clasificador** y devuelve un **accuracy** sin errores.
* La **regresi√≥n lineal** muestra R¬≤, coeficiente, intercepto y una predicci√≥n.
* Se demuestra un flujo completo **ETL ‚Üí ML**.

---

# ‚úÖ Conclusi√≥n global del Laboratorio 10

**Qu√© has hecho:**

1. **Fase 1 (SQLite)** ‚Üí Modelado relacional, CRUD, JOINs, c√°lculos de importe.
2. **Fase 2 (MongoDB)** ‚Üí Modelo documental, CRUD, `lookup` en pipeline, c√°lculo de importe.
3. **Fase 3 (Pandas)** ‚Üí Exportaci√≥n, unificaci√≥n de tablas, agregaciones e informes.
4. **Fase 4 (Scikit-learn)** ‚Üí Preprocesado, clasificaci√≥n binaria, regresi√≥n lineal simple.

**Qu√© te llevas:**

* Experiencia pr√°ctica con **SQL y NoSQL** en paralelo.
* Uso de **Pandas** para ETL y an√°lisis tabular.
* Primer contacto con **ML aplicado a datos reales** (clasificaci√≥n y regresi√≥n).

**Ideas clave:**

* SQL aporta **integridad y consistencia**, MongoDB aporta **flexibilidad y escalado**.
* Pandas es un gran ‚Äúpuente‚Äù entre **datos en bruto** y **modelado ML**.
* Scikit-learn ofrece un flujo claro: **preprocesar ‚Üí entrenar ‚Üí evaluar**.

**Siguientes pasos:**

* Extender el modelo ML con m√°s features (ej. cliente, tipo de producto).
* Probar clustering (KMeans) para segmentaci√≥n de clientes/productos.
* Empaquetar todo en un **CLI** con `argparse` para lanzar fases (`sql seed`, `mongo seed`, `etl`, `ml`).
* A√±adir **tests automatizados** para validar consistencia de datos y accuracy m√≠nimo esperado.